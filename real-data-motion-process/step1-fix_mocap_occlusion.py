#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆåŠ¨æ•æ•°æ®é®æŒ¡å¼‚å¸¸å€¼ä¿®å¤ - Step 1 Improved
æ›´æ™ºèƒ½çš„å¼‚å¸¸å€¼æ£€æµ‹å’Œä¿®å¤ç­–ç•¥
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from datetime import datetime
from scipy.interpolate import interp1d
from scipy.signal import medfilt
from scipy.stats import zscore

def load_csv_data_asap(csv_file):
    """åŠ è½½ASAPæ ¼å¼çš„CSVæ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½CSVæ–‡ä»¶: {os.path.basename(csv_file)}")
    
    # æŸ¥æ‰¾æ•°æ®å¼€å§‹çš„è¡Œ
    with open(csv_file, 'r') as f:
        lines = f.readlines()
    
    start_row = None
    for i, line in enumerate(lines):
        if 'Frame#' in line:
            start_row = i + 1  # è·³è¿‡è¡¨å¤´ï¼Œä»æ•°æ®è¡Œå¼€å§‹
            break
    
    if start_row is None:
        # å°è¯•pandasç›´æ¥è¯»å–
        try:
            df = pd.read_csv(csv_file)
            print(f"âœ… ç›´æ¥åŠ è½½CSVæˆåŠŸ")
        except Exception as e:
            raise ValueError(f"æ— æ³•æ‰¾åˆ°æ•°æ®å¼€å§‹è¡Œä¸”ç›´æ¥è¯»å–å¤±è´¥: {e}")
    else:
        print(f"   è·³è¿‡å‰{start_row}è¡Œå¤´éƒ¨ä¿¡æ¯")
        df = pd.read_csv(csv_file, skiprows=start_row)
    
    # æ£€æŸ¥å¹¶æ¸…ç†æ•°æ®
    print(f"   åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # æå–ä½ç½®æ•°æ® (mm)
    pos_x = df['XToGlobal1'].values
    pos_y = df['YToGlobal1'].values  
    pos_z = df['ZToGlobal1'].values
    
    # æå–é€Ÿåº¦æ•°æ® (mm/s) - å¤„ç†ç©ºå€¼
    vel_x = pd.to_numeric(df['VxToGlobal1'], errors='coerce').values
    vel_y = pd.to_numeric(df['VyToGlobal1'], errors='coerce').values
    vel_z = pd.to_numeric(df['VzToGlobal1'], errors='coerce').values
    
    # å°†ç©ºå€¼æ›¿æ¢ä¸º0
    vel_x = np.nan_to_num(vel_x, nan=0.0)
    vel_y = np.nan_to_num(vel_y, nan=0.0)
    vel_z = np.nan_to_num(vel_z, nan=0.0)
    
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    print(f"   æ€»å¸§æ•°: {len(df)}")
    print(f"   é‡‡æ ·ç‡: 120Hz")
    print(f"   æŒç»­æ—¶é—´: {len(df)/120:.2f}ç§’")
    print(f"   ä½ç½®èŒƒå›´: X[{pos_x.min():.1f}, {pos_x.max():.1f}] Y[{pos_y.min():.1f}, {pos_y.max():.1f}] Z[{pos_z.min():.1f}, {pos_z.max():.1f}] mm")
    
    return df, pos_x, pos_y, pos_z, vel_x, vel_y, vel_z

def detect_occlusion_outliers_improved(pos_data, axis_name, adaptive_thresholds=True):
    """æ”¹è¿›çš„å¼‚å¸¸å€¼æ£€æµ‹"""
    print(f"ğŸ” æ£€æµ‹{axis_name}è½´é®æŒ¡å¼‚å¸¸å€¼ï¼ˆæ”¹è¿›ç‰ˆï¼‰...")
    
    outliers = set()
    
    # è®¡ç®—æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾
    data_range = pos_data.max() - pos_data.min()
    data_std = np.std(pos_data)
    data_median = np.median(pos_data)
    
    # ğŸ”§ æ”¹è¿›1: è‡ªé€‚åº”é˜ˆå€¼
    if adaptive_thresholds:
        # åŸºäºæ•°æ®åˆ†å¸ƒåŠ¨æ€è°ƒæ•´é˜ˆå€¼
        threshold_low = max(1.0, data_std * 0.1)  # åŠ¨æ€ä½å€¼é˜ˆå€¼
        threshold_jump = max(100.0, data_std * 5)  # åŠ¨æ€è·³å˜é˜ˆå€¼
    else:
        threshold_low = 1.0
        threshold_jump = 500.0
    
    print(f"   è‡ªé€‚åº”é˜ˆå€¼: ä½å€¼={threshold_low:.1f}mm, è·³å˜={threshold_jump:.1f}mm")
    
    # æ–¹æ³•1: æ£€æµ‹æ¥è¿‘0çš„å€¼ - æ”¹è¿›ç‰ˆ
    # åªæœ‰å½“æ•°æ®æ˜æ˜¾åç¦»ä¸­å¿ƒä¸”æ¥è¿‘0æ—¶æ‰è®¤ä¸ºæ˜¯å¼‚å¸¸
    if abs(data_median) > threshold_low * 2:  # åªæœ‰å½“æ•°æ®ä¸­å¿ƒä¸åœ¨0é™„è¿‘æ—¶æ‰æ£€æµ‹
        near_zero = np.abs(pos_data) < threshold_low
        zero_outliers = np.where(near_zero)[0]
        outliers.update(zero_outliers)
        print(f"   æ–¹æ³•1(æ¥è¿‘0): æ£€æµ‹åˆ° {len(zero_outliers)} ä¸ªå¼‚å¸¸ç‚¹")
    else:
        print(f"   æ–¹æ³•1(æ¥è¿‘0): è·³è¿‡ï¼Œæ•°æ®ä¸­å¿ƒåœ¨åŸç‚¹é™„è¿‘")
    
    # æ–¹æ³•2: æ£€æµ‹çªç„¶çš„å¤§è·³å˜ - æ”¹è¿›ç‰ˆ
    diff = np.abs(np.diff(pos_data))
    # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼ï¼Œè€ƒè™‘å±€éƒ¨å˜åŒ–
    rolling_std = pd.Series(diff).rolling(window=min(20, len(diff)//5), center=True).std()
    dynamic_threshold = np.maximum(threshold_jump, rolling_std * 3).fillna(threshold_jump)
    # ç¡®ä¿é•¿åº¦åŒ¹é…
    assert len(dynamic_threshold) == len(diff), f"é•¿åº¦ä¸åŒ¹é…: dynamic_threshold={len(dynamic_threshold)}, diff={len(diff)}"
    large_jumps = diff > dynamic_threshold
    
    jump_indices = np.where(large_jumps)[0] + 1
    outliers.update(jump_indices)
    print(f"   æ–¹æ³•2(å¤§è·³å˜): æ£€æµ‹åˆ° {len(jump_indices)} ä¸ªå¼‚å¸¸ç‚¹")
    
    # æ–¹æ³•3: ä½¿ç”¨ä¸­å€¼æ»¤æ³¢æ£€æµ‹å¼‚å¸¸å€¼ - æ”¹è¿›ç‰ˆ
    window_size = min(21, len(pos_data) // 10)
    if window_size % 2 == 0:
        window_size += 1
    
    median_filtered = medfilt(pos_data, kernel_size=window_size)
    residuals = np.abs(pos_data - median_filtered)
    
    # ğŸ”§ æ”¹è¿›2: ä½¿ç”¨æ›´ä¿å®ˆçš„é˜ˆå€¼
    threshold_med = np.median(residuals) + 4 * np.std(residuals)  # ä»3å€æ”¹ä¸º4å€
    median_outliers = np.where(residuals > threshold_med)[0]
    outliers.update(median_outliers)
    print(f"   æ–¹æ³•3(ä¸­å€¼æ»¤æ³¢): æ£€æµ‹åˆ° {len(median_outliers)} ä¸ªå¼‚å¸¸ç‚¹")
    
    # ğŸ”§ æ”¹è¿›3: åå¤„ç†è¿‡æ»¤
    outliers = np.array(sorted(outliers))
    
    # è¿‡æ»¤æ‰å­¤ç«‹çš„å•ç‚¹å¼‚å¸¸ï¼ˆå¯èƒ½æ˜¯æ­£å¸¸çš„å¿«é€ŸåŠ¨ä½œï¼‰
    if len(outliers) > 0:
        # è®¡ç®—æ¯ä¸ªå¼‚å¸¸ç‚¹çš„é‚»åŸŸå¯†åº¦
        isolated_outliers = []
        for i, outlier_idx in enumerate(outliers):
            # æ£€æŸ¥å‰å5ä¸ªç‚¹çš„èŒƒå›´å†…æ˜¯å¦æœ‰å…¶ä»–å¼‚å¸¸ç‚¹
            neighbor_range = 5
            neighbors = outliers[
                (outliers >= outlier_idx - neighbor_range) & 
                (outliers <= outlier_idx + neighbor_range) &
                (outliers != outlier_idx)
            ]
            if len(neighbors) == 0:  # å­¤ç«‹ç‚¹
                isolated_outliers.append(outlier_idx)
        
        if isolated_outliers:
            print(f"   è¿‡æ»¤å­¤ç«‹å¼‚å¸¸ç‚¹: {len(isolated_outliers)} ä¸ª")
            outliers = np.setdiff1d(outliers, isolated_outliers)
    
    print(f"   æœ€ç»ˆæ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸ç‚¹")
    print(f"   å¼‚å¸¸ç‚¹æ¯”ä¾‹: {len(outliers)/len(pos_data)*100:.2f}%")
    
    if len(outliers) > 0:
        print(f"   å¼‚å¸¸å€¼èŒƒå›´: [{pos_data[outliers].min():.1f}, {pos_data[outliers].max():.1f}] mm")
    
    return outliers

def interpolate_outliers_safe(pos_data, outliers, method='cubic'):
    """å®‰å…¨çš„æ’å€¼ä¿®å¤å¼‚å¸¸å€¼"""
    if len(outliers) == 0:
        return pos_data.copy()
    
    print(f"ğŸ”§ ä½¿ç”¨å®‰å…¨{method}æ’å€¼ä¿®å¤å¼‚å¸¸å€¼...")
    
    # åˆ›å»ºä¿®å¤åçš„æ•°æ®å‰¯æœ¬
    fixed_data = pos_data.copy()
    
    # è·å–æœ‰æ•ˆæ•°æ®ç‚¹çš„ç´¢å¼•
    valid_indices = np.setdiff1d(np.arange(len(pos_data)), outliers)
    
    if len(valid_indices) < 2:
        print("âŒ æœ‰æ•ˆæ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæ’å€¼")
        return pos_data.copy()
    
    # è®¡ç®—åˆç†çš„æ•°æ®èŒƒå›´
    data_min = np.percentile(pos_data[valid_indices], 1)   # 1%åˆ†ä½æ•°
    data_max = np.percentile(pos_data[valid_indices], 99)  # 99%åˆ†ä½æ•°
    data_range = data_max - data_min
    
    # åˆ›å»ºæ’å€¼å‡½æ•°
    try:
        if method == 'cubic' and len(valid_indices) >= 4:
            # ğŸ”§ æ”¹è¿›4: é™åˆ¶å¤–æ¨èŒƒå›´
            interp_func = interp1d(valid_indices, pos_data[valid_indices], 
                                 kind='cubic', bounds_error=False, 
                                 fill_value=(pos_data[valid_indices[0]], pos_data[valid_indices[-1]]))
        else:
            interp_func = interp1d(valid_indices, pos_data[valid_indices], 
                                 kind='linear', bounds_error=False,
                                 fill_value=(pos_data[valid_indices[0]], pos_data[valid_indices[-1]]))
        
        # å¯¹å¼‚å¸¸å€¼è¿›è¡Œæ’å€¼
        interpolated_values = interp_func(outliers)
        
        # ğŸ”§ æ”¹è¿›5: éªŒè¯æ’å€¼ç»“æœ
        # ç¡®ä¿æ’å€¼ç»“æœåœ¨åˆç†èŒƒå›´å†…
        valid_interpolated = (interpolated_values >= data_min - data_range * 0.2) & \
                           (interpolated_values <= data_max + data_range * 0.2)
        
        if not np.all(valid_interpolated):
            print(f"   è­¦å‘Š: {np.sum(~valid_interpolated)} ä¸ªæ’å€¼ç»“æœè¶…å‡ºåˆç†èŒƒå›´ï¼Œä½¿ç”¨æœ€è¿‘é‚»æ›¿ä»£")
            # å¯¹è¶…å‡ºèŒƒå›´çš„å€¼ä½¿ç”¨æœ€è¿‘æœ‰æ•ˆç‚¹çš„å€¼
            for i, outlier_idx in enumerate(outliers):
                if not valid_interpolated[i]:
                    # æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆç‚¹
                    distances = np.abs(valid_indices - outlier_idx)
                    nearest_valid_idx = valid_indices[np.argmin(distances)]
                    interpolated_values[i] = pos_data[nearest_valid_idx]
        
        fixed_data[outliers] = interpolated_values
        
        print(f"âœ… å®‰å…¨æ’å€¼ä¿®å¤å®Œæˆ")
        print(f"   ä¿®å¤å‰èŒƒå›´: [{pos_data.min():.1f}, {pos_data.max():.1f}] mm")
        print(f"   ä¿®å¤åèŒƒå›´: [{fixed_data.min():.1f}, {fixed_data.max():.1f}] mm")
        
        # éªŒè¯ä¿®å¤è´¨é‡
        if len(outliers) > 0:
            max_change = np.max(np.abs(fixed_data[outliers] - pos_data[outliers]))
            print(f"   æœ€å¤§ä¿®å¤å˜åŒ–: {max_change:.1f} mm")
        
    except Exception as e:
        print(f"âŒ æ’å€¼å¤±è´¥: {e}")
        return pos_data.copy()
    
    return fixed_data

def analyze_outlier_segments(outliers):
    """åˆ†æå¼‚å¸¸å€¼çš„è¿ç»­æ®µ"""
    if len(outliers) == 0:
        return []
    
    segments = []
    start = outliers[0]
    
    for i in range(1, len(outliers)):
        # å¦‚æœä¸è¿ç»­ï¼Œç»“æŸå½“å‰æ®µ
        if outliers[i] - outliers[i-1] > 1:
            segments.append((start, outliers[i-1]))
            start = outliers[i]
    
    # æ·»åŠ æœ€åä¸€æ®µ
    segments.append((start, outliers[-1]))
    
    print(f"   å¼‚å¸¸å€¼åˆ†ä¸º {len(segments)} ä¸ªè¿ç»­æ®µ:")
    for i, (s, e) in enumerate(segments):
        print(f"     æ®µ{i+1}: [{s}, {e}] é•¿åº¦={e-s+1}")
    
    return segments

def visualize_fix_results(original_pos, fixed_pos, outliers, axis_name, save_path=None):
    """å¯è§†åŒ–ä¿®å¤ç»“æœ"""
    print(f"ğŸ“Š ç”Ÿæˆ{axis_name}è½´ä¿®å¤å¯è§†åŒ–...")
    
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))
    
    time_axis = np.arange(len(original_pos)) / 120.0  # 120Hz
    
    # å­å›¾1: åŸå§‹æ•°æ®å’Œå¼‚å¸¸å€¼
    ax1.plot(time_axis, original_pos, 'b-', linewidth=1, alpha=0.7, label='Original Data')
    if len(outliers) > 0:
        ax1.scatter(time_axis[outliers], original_pos[outliers], c='red', s=20, label=f'Outliers ({len(outliers)} points)')
    ax1.set_title(f'{axis_name}-axis Position Data - Outlier Detection (Improved)', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Position (mm)')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # å­å›¾2: ä¿®å¤åçš„æ•°æ®
    ax2.plot(time_axis, original_pos, 'b-', linewidth=1, alpha=0.5, label='Original Data')
    ax2.plot(time_axis, fixed_pos, 'g-', linewidth=2, label='Fixed Data')
    if len(outliers) > 0:
        ax2.scatter(time_axis[outliers], fixed_pos[outliers], c='orange', s=20, label=f'Interpolated Points ({len(outliers)} points)')
    ax2.set_title(f'{axis_name}-axis Position Data - Fix Results (Improved)', fontsize=14, fontweight='bold')
    ax2.set_ylabel('Position (mm)')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    # å­å›¾3: ä¿®å¤å·®å¼‚
    diff = fixed_pos - original_pos
    ax3.plot(time_axis, diff, 'r-', linewidth=1, label='Fix Difference')
    ax3.axhline(y=0, color='k', linestyle='--', alpha=0.5)
    ax3.set_title(f'{axis_name}-axis Fix Difference (Improved)', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Time (s)')
    ax3.set_ylabel('Difference (mm)')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    
    plt.tight_layout()
    
    if save_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = f"{timestamp}_mocap_fix_improved_{axis_name}.png"
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š Visualization saved: {save_path}")
    plt.close()
    
    return save_path

def fix_velocity_data(vel_data, pos_data_fixed, dt=1/120.0):
    """åŸºäºä¿®å¤åçš„ä½ç½®æ•°æ®é‡æ–°è®¡ç®—é€Ÿåº¦"""
    print(f"ğŸ”§ é‡æ–°è®¡ç®—é€Ÿåº¦æ•°æ®...")
    
    # ä½¿ç”¨ä¸­å¿ƒå·®åˆ†è®¡ç®—é€Ÿåº¦
    vel_fixed = np.gradient(pos_data_fixed, dt)
    
    print(f"   åŸå§‹é€Ÿåº¦èŒƒå›´: [{vel_data.min():.1f}, {vel_data.max():.1f}] mm/s")
    print(f"   ä¿®å¤åé€Ÿåº¦èŒƒå›´: [{vel_fixed.min():.1f}, {vel_fixed.max():.1f}] mm/s")
    
    return vel_fixed

def process_single_csv(csv_file, csv_output_dir, vis_output_dir):
    """å¤„ç†å•ä¸ªCSVæ–‡ä»¶"""
    print(f"\nğŸ¯ å¤„ç†æ–‡ä»¶: {os.path.basename(csv_file)}")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“Š ç¬¬1æ­¥: åŠ è½½æ•°æ®")
    df, pos_x, pos_y, pos_z, vel_x, vel_y, vel_z = load_csv_data_asap(csv_file)
    
    # 2. æ”¹è¿›çš„å¼‚å¸¸å€¼æ£€æµ‹
    print("\nğŸ” ç¬¬2æ­¥: æ”¹è¿›çš„å¼‚å¸¸å€¼æ£€æµ‹")
    outliers_x = detect_occlusion_outliers_improved(pos_x, 'X')
    outliers_y = detect_occlusion_outliers_improved(pos_y, 'Y')
    outliers_z = detect_occlusion_outliers_improved(pos_z, 'Z')
    
    # åˆ†æå¼‚å¸¸å€¼æ®µ
    if len(outliers_x) > 0:
        print(f"\nğŸ“ˆ Xè½´å¼‚å¸¸å€¼åˆ†æ:")
        analyze_outlier_segments(outliers_x)
    
    if len(outliers_y) > 0:
        print(f"\nğŸ“ˆ Yè½´å¼‚å¸¸å€¼åˆ†æ:")
        analyze_outlier_segments(outliers_y)
    
    if len(outliers_z) > 0:
        print(f"\nğŸ“ˆ Zè½´å¼‚å¸¸å€¼åˆ†æ:")
        analyze_outlier_segments(outliers_z)
    
    # 3. å®‰å…¨æ’å€¼ä¿®å¤
    print("\nğŸ”§ ç¬¬3æ­¥: å®‰å…¨æ’å€¼ä¿®å¤")
    pos_x_fixed = interpolate_outliers_safe(pos_x, outliers_x)
    pos_y_fixed = interpolate_outliers_safe(pos_y, outliers_y)
    pos_z_fixed = interpolate_outliers_safe(pos_z, outliers_z)
    
    # 4. é‡æ–°è®¡ç®—é€Ÿåº¦
    print("\nâš¡ ç¬¬4æ­¥: é‡æ–°è®¡ç®—é€Ÿåº¦")
    vel_x_fixed = fix_velocity_data(vel_x, pos_x_fixed)
    vel_y_fixed = fix_velocity_data(vel_y, pos_y_fixed)
    vel_z_fixed = fix_velocity_data(vel_z, pos_z_fixed)
    
    # 5. ç”Ÿæˆå¯è§†åŒ–ï¼ˆåªä¿å­˜Yè½´ï¼‰
    print("\nğŸ“Š ç¬¬5æ­¥: ç”ŸæˆYè½´å¯è§†åŒ–")
    vis_paths = []
    base_name = os.path.basename(csv_file).replace('.csv', '')
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åªä¸ºYè½´ç”Ÿæˆå¯è§†åŒ–
    vis_path = os.path.join(vis_output_dir, f"{timestamp}_step1_improved_mocap_fix_{base_name}_Y.png")
    vis_paths.append(visualize_fix_results(pos_y, pos_y_fixed, outliers_y, 'Y', vis_path))
    
    # 6. ä¿å­˜ä¿®å¤åçš„æ•°æ®
    print("\nğŸ’¾ ç¬¬6æ­¥: ä¿å­˜ä¿®å¤åçš„æ•°æ®")
    
    # åˆ›å»ºä¿®å¤åçš„DataFrame
    df_fixed = df.copy()
    df_fixed['XToGlobal1'] = pos_x_fixed
    df_fixed['YToGlobal1'] = pos_y_fixed
    df_fixed['ZToGlobal1'] = pos_z_fixed
    df_fixed['VxToGlobal1'] = vel_x_fixed
    df_fixed['VyToGlobal1'] = vel_y_fixed
    df_fixed['VzToGlobal1'] = vel_z_fixed
    
    # ä¿å­˜æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_name = os.path.basename(csv_file).replace('.csv', '')
    output_file = f"{timestamp}_step1_improved_fixed_{base_name}.csv"
    output_path = os.path.join(csv_output_dir, output_file)
    
    df_fixed.to_csv(output_path, index=False)
    
    print(f"ğŸ’¾ æ”¹è¿›ä¿®å¤åçš„CSVå·²ä¿å­˜: {output_file}")
    
    # 7. è¾“å‡ºç»Ÿè®¡æ‘˜è¦
    print(f"\nâœ… {os.path.basename(csv_file)} ä¿®å¤å®Œæˆ!")
    print("ğŸ“Š ä¿®å¤ç»Ÿè®¡:")
    print(f"   Xè½´å¼‚å¸¸å€¼: {len(outliers_x)} ä¸ª")
    print(f"   Yè½´å¼‚å¸¸å€¼: {len(outliers_y)} ä¸ª")
    print(f"   Zè½´å¼‚å¸¸å€¼: {len(outliers_z)} ä¸ª")
    print(f"   æ€»å¼‚å¸¸å€¼: {len(set(outliers_x) | set(outliers_y) | set(outliers_z))} ä¸ª")
    print(f"   æ•°æ®å®Œæ•´æ€§: {(1 - len(set(outliers_x) | set(outliers_y) | set(outliers_z)) / len(df)) * 100:.1f}%")
    
    if vis_paths:
        print(f"ğŸ“Š å¯è§†åŒ–æ–‡ä»¶: {len(vis_paths)} ä¸ª")
        for path in vis_paths:
            print(f"   - {os.path.basename(path)}")
    
    print(f"ğŸ“ ä¿®å¤åCSVæ–‡ä»¶: {output_file}")
    print(f"ğŸ“‚ CSVä¿å­˜ä½ç½®: {csv_output_dir}")
    print(f"ğŸ“Š å¯è§†åŒ–ä¿å­˜ä½ç½®: {vis_output_dir}")
    
    return {
        'csv_file': csv_file,
        'output_file': output_file,
        'outliers_x': len(outliers_x),
        'outliers_y': len(outliers_y),
        'outliers_z': len(outliers_z),
        'total_outliers': len(set(outliers_x) | set(outliers_y) | set(outliers_z)),
        'data_integrity': (1 - len(set(outliers_x) | set(outliers_y) | set(outliers_z)) / len(df)) * 100,
        'vis_paths': vis_paths,
        'success': True
    }

def main():
    # è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
    input_dir = "/home/user/pbhc-main- cqh723/pbhc-main/final-aligine/8.1-csv"
    
    # è¾“å‡ºç›®å½•
    base_output_dir = "/home/user/pbhc-main- cqh723/pbhc-main/real-data-motion-process/output"
    csv_output_dir = os.path.join(base_output_dir, "fixed_csv")  # CSVæ–‡ä»¶å¤¹
    vis_output_dir = os.path.join(base_output_dir, "visualizations")  # å¯è§†åŒ–æ–‡ä»¶å¤¹
    
    if not os.path.exists(input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(csv_output_dir, exist_ok=True)
    os.makedirs(vis_output_dir, exist_ok=True)
    
    # æ‰¾åˆ°æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = []
    for file in os.listdir(input_dir):
        if file.lower().endswith('.csv'):
            csv_files.append(os.path.join(input_dir, file))
    
    if not csv_files:
        print(f"âŒ åœ¨ç›®å½• {input_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    print("ğŸ¯ æ‰¹é‡åŠ¨æ•æ•°æ®é®æŒ¡å¼‚å¸¸å€¼ä¿®å¤ - Step 1 Improved")
    print("=" * 80)
    print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {input_dir}")
    print(f"ğŸ“‚ CSVè¾“å‡ºç›®å½•: {csv_output_dir}")
    print(f"ğŸ“Š å¯è§†åŒ–è¾“å‡ºç›®å½•: {vis_output_dir}")
    print(f"ğŸ“„ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶:")
    for i, csv_file in enumerate(csv_files, 1):
        print(f"   {i}. {os.path.basename(csv_file)}")
    
    # æ‰¹é‡å¤„ç†
    results = []
    successful_count = 0
    failed_count = 0
    
    for i, csv_file in enumerate(csv_files, 1):
        try:
            print(f"\n{'='*20} å¤„ç†è¿›åº¦: {i}/{len(csv_files)} {'='*20}")
            result = process_single_csv(csv_file, csv_output_dir, vis_output_dir)
            results.append(result)
            if result['success']:
                successful_count += 1
            else:
                failed_count += 1
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {os.path.basename(csv_file)} æ—¶å‡ºé”™: {e}")
            failed_count += 1
            results.append({
                'csv_file': csv_file,
                'success': False,
                'error': str(e)
            })
    
    # è¾“å‡ºæ‰¹é‡å¤„ç†æ€»ç»“
    print(f"\n{'='*80}")
    print("ğŸŠ æ‰¹é‡å¤„ç†å®Œæˆ!")
    print("ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»æ–‡ä»¶æ•°: {len(csv_files)}")
    print(f"   âœ… æˆåŠŸå¤„ç†: {successful_count}")
    print(f"   âŒ å¤„ç†å¤±è´¥: {failed_count}")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {successful_count/len(csv_files)*100:.1f}%")
    
    if successful_count > 0:
        print(f"\nğŸ“Š æˆåŠŸå¤„ç†æ–‡ä»¶çš„è¯¦ç»†ç»Ÿè®¡:")
        total_outliers = 0
        total_vis_files = 0
        avg_integrity = 0
        
        for result in results:
            if result['success']:
                total_outliers += result['total_outliers']
                total_vis_files += len(result['vis_paths'])
                avg_integrity += result['data_integrity']
                print(f"   ğŸ“„ {os.path.basename(result['csv_file'])}: "
                      f"å¼‚å¸¸å€¼={result['total_outliers']}, "
                      f"å®Œæ•´æ€§={result['data_integrity']:.1f}%, "
                      f"å¯è§†åŒ–={len(result['vis_paths'])}ä¸ª")
        
        avg_integrity /= successful_count
        print(f"\nğŸ“ˆ æ±‡æ€»ç»Ÿè®¡:")
        print(f"   æ€»å¼‚å¸¸å€¼ä¿®å¤: {total_outliers} ä¸ª")
        print(f"   æ€»å¯è§†åŒ–æ–‡ä»¶: {total_vis_files} ä¸ª")
        print(f"   å¹³å‡æ•°æ®å®Œæ•´æ€§: {avg_integrity:.1f}%")
    
    if failed_count > 0:
        print(f"\nâŒ å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
        for result in results:
            if not result['success']:
                print(f"   - {os.path.basename(result['csv_file'])}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print(f"\nğŸ“‚ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ä¿å­˜åœ¨:")
    print(f"   ğŸ“ CSVæ–‡ä»¶: {csv_output_dir}")
    print(f"   ğŸ“Š å¯è§†åŒ–æ–‡ä»¶: {vis_output_dir}")
    print("ğŸ‰ æ‰¹é‡å¤„ç†ä»»åŠ¡å®Œæˆ!")

if __name__ == "__main__":
    main() 